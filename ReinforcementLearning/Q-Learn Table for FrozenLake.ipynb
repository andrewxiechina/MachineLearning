{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This example implements a q-learn table for FrozenLake in gym. \n",
    "\n",
    "The final score is \n",
    "0.7 ~ 0.8 /0.78\n",
    "\n",
    "It is l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game\n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "rozenLake-v0 defines \"solving\" as getting average reward of 0.78 over 100 consecutive trials.\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "**GAME SURFACE**\n",
    "\n",
    "SFFF<br />\n",
    "FHFH<br />\n",
    "FFFH<br />\n",
    "HFFG\n",
    "\n",
    "**NOTES**\n",
    "\n",
    "S: starting point, safe<br />\n",
    "F: frozen surface, safe<br />\n",
    "H: hole, fall to your doom<br />\n",
    "G: goal, where the frisbee is located\n",
    "\n",
    "**ACTIONS**\n",
    "\n",
    "0: Left<br />\n",
    "1: Down<br />\n",
    "2: Right<br />\n",
    "3: Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_o = env.observation_space.n # Observation space = 16, number of different states\n",
    "n_a = env.action_space.n # Action space = 4\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "Q # Q-Table is a 16 * 4 matrix, with each value representing the reward if the action is taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = .8\n",
    "gamma = .95 # Future reward is discounted accordingly\n",
    "n_epochs = 2000 # Number of games for training\n",
    "n_steps = 199 # Number of steps every game plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_epochs):\n",
    "    state = env.reset() # s is reset to 0, the starting point\n",
    "    done = False\n",
    "    \n",
    "    for j in range(n_steps):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Table learning algorithm\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            #Update Q-Table with new knowledge\n",
    "            Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.472\n"
     ]
    }
   ],
   "source": [
    "print(\"Score over time: \" +  str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Table Values\n",
      "[[  2.14845199e-01   3.97555433e-03   3.19736198e-03   1.62249310e-03]\n",
      " [  1.57151716e-04   6.97870516e-05   1.78712050e-04   4.65635054e-01]\n",
      " [  2.32241127e-04   1.84659132e-01   6.32643971e-04   2.99464033e-03]\n",
      " [  2.95287477e-04   6.17121299e-04   6.20370903e-05   8.90514420e-02]\n",
      " [  3.64614858e-01   3.11022462e-04   2.90867757e-04   5.07104797e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.21669362e-03   4.79870701e-05   4.97399573e-02   1.71792431e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  4.19117818e-04   1.23306929e-03   1.23309570e-03   5.97388958e-01]\n",
      " [  2.83277841e-05   5.90799416e-01   3.55913137e-04   2.09160613e-04]\n",
      " [  8.03490695e-01   3.32934009e-04   1.08388031e-04   7.93252601e-05]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  5.51702651e-03   1.94147283e-03   4.51633226e-01   1.73108652e-04]\n",
      " [  3.61766855e-02   9.79286363e-01   1.65653111e-02   2.58579300e-02]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Q-Table Values\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73\n"
     ]
    }
   ],
   "source": [
    "n_trails = 100\n",
    "reward_counter = 0\n",
    "for i in range(n_trails):\n",
    "    state = env.reset() # s is reset to 0, the starting point\n",
    "    done = False\n",
    "    for j in range(n_steps):\n",
    "        reward_prediction = Q[state,:]\n",
    "        action = np.argmax(reward_prediction)\n",
    "        \n",
    "        state_next, reward, done, info = env.step(action)\n",
    "                \n",
    "        # Prepare for the next state\n",
    "        reward_counter = reward_counter + reward\n",
    "        state = state_next   \n",
    "        if done:\n",
    "            break\n",
    "score = float(reward_counter) / n_trails\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Q[0,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
