{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "This exercise experiments on 1 hidden layer (size = 1024) with full data.\n",
    "Dropout does improve accuracy, the improvement is very significant.\n",
    "\n",
    "**Senario without dropout: 1024 * 1 Hidden layer**\n",
    "- validation: 0.9198\n",
    "- test: 0.9038\n",
    "\n",
    "** BEST: ** **256*3 Hidden Layer with Early Stopping**\n",
    "- validation: 0.9166\n",
    "- test: 0.9087\n",
    "\n",
    "**256*5 Hidden Layer with Early Stopping**\n",
    "- validation: 0.9119\n",
    "- test: 0.9044\n",
    "\n",
    "\n",
    "**256*5 Hidden Layer with Early Stopping (patients 2->3)**\n",
    "- validation: 0.9131\n",
    "- test: 0.9092\n",
    "\n",
    "**512*5 Hidden Layer with Early Stopping **\n",
    "- validation: 0.9144\n",
    "- test: 0.9060\n",
    "\n",
    "**512*5 Hidden Layer with Early Stopping (Dropout 0.2 -> 0.5)**\n",
    "- validation: 0.9160\n",
    "- test: 0.9049\n",
    "\n",
    "**512*7 Hidden Layer with Early Stopping**\n",
    "- validation: 0.9131\n",
    "- test: 0.9028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input, Flatten, Reshape, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_12 (Reshape)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 335,114\n",
      "Trainable params: 335,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "n = 256\n",
    "\n",
    "model.add(Reshape((28*28,), input_shape=(28,28) ))\n",
    "\n",
    "model.add(Dense(units=n))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=n))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=n))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='sgd',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_validate, y_validate, X_test, y_test = np.load(\"/Users/andyxie/workspace/input/data_all_flat.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11290 samples, validate on 5561 samples\n",
      "Epoch 1/100\n",
      "11290/11290 [==============================] - 1s - loss: 1.6364 - acc: 0.4759 - val_loss: 0.8849 - val_acc: 0.8020\n",
      "Epoch 2/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.8601 - acc: 0.7432 - val_loss: 0.5644 - val_acc: 0.8498\n",
      "Epoch 3/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.6460 - acc: 0.8074 - val_loss: 0.4830 - val_acc: 0.8633\n",
      "Epoch 4/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.5691 - acc: 0.8307 - val_loss: 0.4455 - val_acc: 0.8720\n",
      "Epoch 5/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.5162 - acc: 0.8453 - val_loss: 0.4241 - val_acc: 0.8781\n",
      "Epoch 6/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.4923 - acc: 0.8524 - val_loss: 0.4081 - val_acc: 0.8829\n",
      "Epoch 7/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.4567 - acc: 0.8653 - val_loss: 0.3972 - val_acc: 0.8858\n",
      "Epoch 8/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.4389 - acc: 0.8700 - val_loss: 0.3871 - val_acc: 0.8896\n",
      "Epoch 9/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.4237 - acc: 0.8741 - val_loss: 0.3781 - val_acc: 0.8914\n",
      "Epoch 10/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.4143 - acc: 0.8787 - val_loss: 0.3720 - val_acc: 0.8926\n",
      "Epoch 11/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.3946 - acc: 0.8812 - val_loss: 0.3688 - val_acc: 0.8914\n",
      "Epoch 12/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.3853 - acc: 0.8848 - val_loss: 0.3637 - val_acc: 0.8953\n",
      "Epoch 13/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.3752 - acc: 0.8864 - val_loss: 0.3564 - val_acc: 0.8964\n",
      "Epoch 14/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.3692 - acc: 0.8891 - val_loss: 0.3530 - val_acc: 0.9002\n",
      "Epoch 15/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.3603 - acc: 0.8932 - val_loss: 0.3502 - val_acc: 0.9002\n",
      "Epoch 16/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.3494 - acc: 0.8966 - val_loss: 0.3476 - val_acc: 0.9007\n",
      "Epoch 17/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.3436 - acc: 0.8962 - val_loss: 0.3453 - val_acc: 0.9020\n",
      "Epoch 18/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.3347 - acc: 0.9005 - val_loss: 0.3415 - val_acc: 0.9038\n",
      "Epoch 19/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.3255 - acc: 0.9021 - val_loss: 0.3401 - val_acc: 0.9033\n",
      "Epoch 20/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.3229 - acc: 0.9030 - val_loss: 0.3372 - val_acc: 0.9043\n",
      "Epoch 21/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.3186 - acc: 0.8997 - val_loss: 0.3352 - val_acc: 0.9056\n",
      "Epoch 22/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.3131 - acc: 0.9050 - val_loss: 0.3336 - val_acc: 0.9063\n",
      "Epoch 23/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.3018 - acc: 0.9099 - val_loss: 0.3318 - val_acc: 0.9076\n",
      "Epoch 24/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.3000 - acc: 0.9103 - val_loss: 0.3295 - val_acc: 0.9083\n",
      "Epoch 25/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.3013 - acc: 0.9098 - val_loss: 0.3288 - val_acc: 0.9079\n",
      "Epoch 26/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2918 - acc: 0.9116 - val_loss: 0.3279 - val_acc: 0.9090\n",
      "Epoch 27/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2876 - acc: 0.9142 - val_loss: 0.3252 - val_acc: 0.9083\n",
      "Epoch 28/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2791 - acc: 0.9145 - val_loss: 0.3256 - val_acc: 0.9097\n",
      "Epoch 29/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2843 - acc: 0.9136 - val_loss: 0.3230 - val_acc: 0.9086\n",
      "Epoch 30/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2777 - acc: 0.9164 - val_loss: 0.3234 - val_acc: 0.9094\n",
      "Epoch 31/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2682 - acc: 0.9187 - val_loss: 0.3212 - val_acc: 0.9104\n",
      "Epoch 32/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2727 - acc: 0.9169 - val_loss: 0.3196 - val_acc: 0.9119\n",
      "Epoch 33/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2685 - acc: 0.9205 - val_loss: 0.3199 - val_acc: 0.9104\n",
      "Epoch 34/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2563 - acc: 0.9235 - val_loss: 0.3207 - val_acc: 0.9110\n",
      "Epoch 35/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2534 - acc: 0.9212 - val_loss: 0.3181 - val_acc: 0.9115\n",
      "Epoch 36/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2531 - acc: 0.9213 - val_loss: 0.3177 - val_acc: 0.9104\n",
      "Epoch 37/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2501 - acc: 0.9252 - val_loss: 0.3166 - val_acc: 0.9110\n",
      "Epoch 38/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2446 - acc: 0.9249 - val_loss: 0.3154 - val_acc: 0.9128\n",
      "Epoch 39/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2456 - acc: 0.9247 - val_loss: 0.3178 - val_acc: 0.9122\n",
      "Epoch 40/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2451 - acc: 0.9233 - val_loss: 0.3149 - val_acc: 0.9133\n",
      "Epoch 41/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2383 - acc: 0.9274 - val_loss: 0.3154 - val_acc: 0.9130\n",
      "Epoch 42/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2393 - acc: 0.9264 - val_loss: 0.3131 - val_acc: 0.9148\n",
      "Epoch 43/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2320 - acc: 0.9295 - val_loss: 0.3138 - val_acc: 0.9151\n",
      "Epoch 44/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2284 - acc: 0.9322 - val_loss: 0.3136 - val_acc: 0.9155\n",
      "Epoch 45/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2256 - acc: 0.9313 - val_loss: 0.3138 - val_acc: 0.9146\n",
      "Epoch 46/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2220 - acc: 0.9323 - val_loss: 0.3117 - val_acc: 0.9157\n",
      "Epoch 47/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2193 - acc: 0.9303 - val_loss: 0.3127 - val_acc: 0.9157\n",
      "Epoch 48/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2181 - acc: 0.9314 - val_loss: 0.3096 - val_acc: 0.9151\n",
      "Epoch 49/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2126 - acc: 0.9353 - val_loss: 0.3111 - val_acc: 0.9160\n",
      "Epoch 50/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2118 - acc: 0.9344 - val_loss: 0.3123 - val_acc: 0.9153\n",
      "Epoch 51/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2103 - acc: 0.9344 - val_loss: 0.3114 - val_acc: 0.9162\n",
      "Epoch 52/100\n",
      "11290/11290 [==============================] - 1s - loss: 0.2079 - acc: 0.9345 - val_loss: 0.3104 - val_acc: 0.9164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a21657828>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data = (X_validate, y_validate),\n",
    "    batch_size = 100,\n",
    "    epochs = 100,\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1873 [========================>.....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.90763481049631856"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, metrics = model.evaluate(X_test, y_test)\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
